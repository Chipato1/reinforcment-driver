{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5eQU7pJxQfod"
   },
   "source": [
    "# COMP47590 Advanced Machine Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rPgBaQLxQfoj"
   },
   "source": [
    "## Assignment 2: Going the Distance\n",
    "Uses the PPO actor-critic method to train a neural network to control a simple robot in the RacingCar environment from OpenAI gym (https://gym.openai.com/envs/RacingCar-v0/). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NZNiGMqsQfok"
   },
   "source": [
    "![Racing](racing_car.gif)\n",
    "\n",
    "There are five discrete **actions** in this environment:\n",
    "- left (0)\n",
    "- right (1)\n",
    "- brake (2)\n",
    "- accelerate (3)\n",
    "- none (4)\n",
    "\n",
    "**Reward** of -0.1 is awarded every frame and +1000/N for every track tile visited, where N is the total number of tiles in track. For example, if you have finished in 732 frames, your reward is 1000 - 0.1*732 = 926.8 points.\n",
    "\n",
    "And the **state** is represented using a single image frame (96 * 96)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:blue\">\n",
    "    \n",
    "## Foreword: How we conducted the work for this assingment\n",
    "    \n",
    "- Use monitor to show it in colab and save mp4 to share\n",
    "- Work on different machines - Exchange ZIP files\n",
    "- We will highlight our texts in this notebook in **blue**\n",
    "\n",
    "First we started working with Google Collab but we ran into some problems, specially with the display. We solved this by using a monitor wrapper and saving the result in an MP4 file, but we finally decided to move back to local executions to find the best. In each section we go more in detail into how we worked with each of the tasks.\n",
    "\n",
    "For the final evaluation, we tested 4 different models (using a single or 4 stacked images and MLP or CNN policy) on two different laptops, this is explained further in detail in the evaluation section, were we compare the resulting best models.\n",
    "\n",
    "\n",
    "\n",
    "</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Dm_TwuoRQfol"
   },
   "source": [
    "## Initialisation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VHWFxiANQfom"
   },
   "source": [
    "If using Google colab you need to install packages - comment out lines below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0XEhFeXkQfom",
    "outputId": "42e5ffd2-1cd3-4daf-b535-82e7c00c2dde"
   },
   "outputs": [],
   "source": [
    "## We used this in the colab environment\n",
    "\n",
    "#!apt install swig cmake ffmpeg\n",
    "#!apt-get install -y xvfb x11-utils\n",
    "#!pip install stable-baselines3[extra] pyglet box2d box2d-kengz\n",
    "#!pip install pyvirtualdisplay PyOpenGL PyOpenGL-accelerate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mV9IM5FRQfoo"
   },
   "source": [
    "For Google colab comment out this cell to make a virtual rendering canvas so render calls work (we still won't see display!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zh2cw7iSQfop"
   },
   "outputs": [],
   "source": [
    "#import pyvirtualdisplay\n",
    "#\n",
    "#_display = pyvirtualdisplay.Display(visible=False,  # use False with Xvfb\n",
    "#                                    size=(1400, 900))\n",
    "#_ = _display.start()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "S8Bcx9GkQfoq"
   },
   "source": [
    "Import required packages. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "dZcWCINuQfor"
   },
   "outputs": [],
   "source": [
    "import torch \n",
    "import gym\n",
    "import stable_baselines3 as sb3\n",
    "\n",
    "import pandas as pd # For data frames and data frame manipulation\n",
    "pd.set_option('display.max_rows', 100)\n",
    "pd.set_option('display.max_columns', 100)\n",
    "pd.set_option('display.width', 1000)\n",
    "import numpy as np # For general  numeric operations\n",
    "import os\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports for MP4 rendering\n",
    "import io\n",
    "import base64\n",
    "from IPython.display import HTML\n",
    "from gym import wrappers\n",
    "\n",
    "# Imports for inline Tensorboard\n",
    "%load_ext tensorboard\n",
    "import datetime, os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In colab we ensure to use the GPU for training\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Here comes all the logic for showing the mp4s\n",
    "\n",
    "def show_render_result(rend_env):\n",
    "  video = io.open('./gym-results/openaigym.video.%s.video000000.mp4' % rend_env.file_infix, 'r+b').read()\n",
    "  encoded = base64.b64encode(video)\n",
    "  return HTML(data=''' \n",
    "  <video width=\"720\" height=\"auto\" alt=\"test\" controls><source src=\"data:video/mp4;base64,{0}\" type=\"video/mp4\" /></video>'''\n",
    "  .format(encoded.decode('ascii')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GEJsAysfQfos"
   },
   "source": [
    "## Create and Explore the Environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CCMqLB1VQfot"
   },
   "source": [
    "Create the **CarRacing-v0** environment. Add wrappers to resize the images and convert to greyscale."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "w7xkqRzcQfot",
    "outputId": "4a53b8ef-2971-4743-96b6-5172c8693737"
   },
   "outputs": [],
   "source": [
    "env = gym.make('CarRacing-v0')\n",
    "env = gym.wrappers.resize_observation.ResizeObservation(env, 64)\n",
    "env = gym.wrappers.gray_scale_observation.GrayScaleObservation(env, keep_dim = True)\n",
    "\n",
    "# This is the env we use to monitor when we want a video of the agent\n",
    "render_env =  wrappers.Monitor(env, \"./gym-results\", force=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "F9BQTzRKQfou"
   },
   "source": [
    "Explore the environment - view the action space and observation space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "peo_ntRkQfou",
    "outputId": "e9170d3a-93e5-4305-89b8-6719ef0f4bde"
   },
   "outputs": [],
   "source": [
    "print(\"action_space: \", env.action_space)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZgObNy4uQfov",
    "outputId": "f319f83d-c61b-47e8-ccac-99e5b39fe4f5"
   },
   "outputs": [],
   "source": [
    "print(\"env.observation_space\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t-QsvRe0Qfow"
   },
   "source": [
    "Play an episode of the environment using random actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "f_jvIhrjQfox",
    "outputId": "f12cbfa7-23a2-4efd-b58f-4729be5eb558"
   },
   "outputs": [],
   "source": [
    "obs = env.reset()\n",
    "done = False\n",
    "\n",
    "while not done:\n",
    "    action = env.action_space.sample()\n",
    "    obs, reward, done, info = env.step(action)\n",
    "    env.render()\n",
    "    env.render('rgb_array')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:blue\">\n",
    "    \n",
    "## Exploration of the Environment\n",
    "    \n",
    "- Write about shapes and wrappers\n",
    "\n",
    "</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2ml3JnJ5Qfox"
   },
   "source": [
    "### Single Image Agent\n",
    "Create an agent that controls the car using a single image frame as the state input. We recommend a PPO agent with the following hyper-parameters (although you can experiment):\n",
    "- learning_rate = 3e-5\n",
    "- n_steps = 512\n",
    "- ent_coef = 0.001\n",
    "- batch_size = 128\n",
    "- gae_lambda =  0.9\n",
    "- n_epochs = 20\n",
    "- use_sde = True\n",
    "- sde_sample_freq = 4\n",
    "- clip_range = 0.4\n",
    "- policy_kwargs = {'log_std_init': -2, 'ortho_init':False},\n",
    "\n",
    "We also recommend enabling **tensorboard** monitoring of the training process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GHY19WhjQfoy",
    "outputId": "38bcf412-18ea-40db-b6b5-b1bff5550f22"
   },
   "outputs": [],
   "source": [
    "tb_log = './tb_logs_SingleFrame_Training/'\n",
    "\n",
    "#policy = 'MlpPolicy'\n",
    "policy = 'CnnPolicy'\n",
    "\n",
    "agent = sb3.PPO(policy, env,\n",
    "                    learning_rate = 3e-5,\n",
    "                    n_steps = 512,\n",
    "                    ent_coef = 0.001,\n",
    "                    batch_size = 128,\n",
    "                    gae_lambda =  0.9,\n",
    "                    n_epochs = 20,\n",
    "                    use_sde = True,\n",
    "                    sde_sample_freq = 4,\n",
    "                    clip_range = 0.4,\n",
    "                    policy_kwargs = {'log_std_init': -2, 'ortho_init':False},\n",
    "                    tensorboard_log=tb_log)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aaVvGXkNQfoz"
   },
   "source": [
    "Examine the actor and critic network architectures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mNjosaimQfoz",
    "outputId": "c64af8eb-3cb9-4d18-f1cd-c3251ea3904a"
   },
   "outputs": [],
   "source": [
    "print(agent.policy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:blue\">\n",
    "    \n",
    "## Training of the Agents\n",
    "    \n",
    "For both of these executions, we ran it using the suggested 500000 timesteps, which took an entire night. The resulting agents are the ones stored in the folders *best_models* which we will go over later in the Evaluation section.\n",
    "\n",
    "In case we want a test run of these, and to show a simple output of how the system is training in our Notebook, the following code and output is simply training all of them with a low number of time_steps (10000). \n",
    "\n",
    "The reason we don't have the output of the best models is that in order to create them we ran the execution on separate python files so that we could execute multiple trainings at the same time.\n",
    "\n",
    "</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pdRpUwTeQfoz"
   },
   "source": [
    "Create an evaluation callback that is called every at regular intervals and renders the episode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "oulh4r78Qfoz",
    "outputId": "876d8029-1ce4-46b7-e338-6643dadb8dd9"
   },
   "outputs": [],
   "source": [
    "eval_env = gym.make('CarRacing-v0')\n",
    "eval_env = gym.wrappers.resize_observation.ResizeObservation(eval_env, 64)\n",
    "eval_env = gym.wrappers.gray_scale_observation.GrayScaleObservation(eval_env, keep_dim = True)\n",
    "\n",
    "# Using MLP policy change: best_model_save_path='./best_model_MLP_Single/'\n",
    "eval_callback = sb3.common.callbacks.EvalCallback(eval_env, \n",
    "                                                  best_model_save_path='./best_model_CNN_Single/',\n",
    "                                                  log_path=tb_log, \n",
    "                                                  eval_freq=5000,\n",
    "                                                  render=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vMj3hpimQfo0"
   },
   "source": [
    "Train the model for a large number of timesteps (500,000 timesteps will probably work well)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%tensorboard --logdir log_racing_PPO_single"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cNtGLDLUQfo0",
    "outputId": "1bc23c12-4702-4676-88b7-c5e7b632c887"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Track generation: 1172..1469 -> 297-tiles track\n",
      "Track generation: 1116..1407 -> 291-tiles track\n",
      "Track generation: 1232..1544 -> 312-tiles track\n",
      "Track generation: 1176..1474 -> 298-tiles track\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/david/.local/lib/python3.8/site-packages/stable_baselines3/common/callbacks.py:345: UserWarning: Training and eval env are not of the same type<stable_baselines3.common.vec_env.vec_transpose.VecTransposeImage object at 0x7fae127458e0> != <stable_baselines3.common.vec_env.dummy_vec_env.DummyVecEnv object at 0x7fae127452e0>\n",
      "  warnings.warn(\"Training and eval env are not of the same type\" f\"{self.training_env} != {self.eval_env}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Track generation: 1271..1593 -> 322-tiles track\n",
      "Track generation: 1043..1312 -> 269-tiles track\n",
      "Track generation: 1167..1463 -> 296-tiles track\n",
      "Track generation: 1120..1408 -> 288-tiles track\n",
      "Track generation: 1225..1536 -> 311-tiles track\n",
      "Track generation: 1140..1435 -> 295-tiles track\n",
      "Track generation: 1168..1464 -> 296-tiles track\n",
      "Track generation: 1316..1649 -> 333-tiles track\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<stable_baselines3.ppo.ppo.PPO at 0x7fae12097f70>"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent.learn(total_timesteps=10000,callback=eval_callback)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Nir4ZmnqQfo1"
   },
   "source": [
    "Save the trained agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "id": "CBGtMv04Qfo1"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/david/.local/lib/python3.8/site-packages/stable_baselines3/common/save_util.py:276: UserWarning: Path 'final_models' does not exist. Will create it.\n",
      "  warnings.warn(f\"Path '{path.parent}' does not exist. Will create it.\")\n"
     ]
    }
   ],
   "source": [
    "# Using MLP policy change to: agent.save(\"./final_models/final_model_MLP_single\")\n",
    "agent.save(\"./final_models/final_model_CNN_single\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And here we can see a quick execution to see how well it performs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_reward, std_reward = sb3.common.evaluation.evaluate_policy(agent, \n",
    "                                                                agent.get_env(), \n",
    "                                                                n_eval_episodes=15,\n",
    "                                                                render = True)\n",
    "print(\"Mean Reward: {} +/- {}\".format(mean_reward, std_reward))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Rxs3h7Owre5T"
   },
   "source": [
    "For memory management delete old agent and environment (assumes variable names - change if required)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uOGiJDrTrdV3"
   },
   "outputs": [],
   "source": [
    "del agent\n",
    "del env\n",
    "del eval_env\n",
    "del render_env"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RQF_FjMEQfo1"
   },
   "source": [
    "### Create Image Stack Agent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the CarRacing-v0 environment using wrappers to resize the images to 64 x 64 and change to greyscale. Also add a wrapper to create a stack of 4 frames. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "id": "ijzQwZY0Qfo2"
   },
   "outputs": [],
   "source": [
    "# Create Stacked env\n",
    "env = gym.make('CarRacing-v0')\n",
    "env = gym.wrappers.resize_observation.ResizeObservation(env, 64)\n",
    "env = gym.wrappers.gray_scale_observation.GrayScaleObservation(env, keep_dim = True)\n",
    "env = sb3.common.vec_env.DummyVecEnv([lambda: env]) \n",
    "env = sb3.common.vec_env.VecFrameStack(env, n_stack=4)\n",
    "\n",
    "# Separate evaluation env\n",
    "eval_env = gym.make('CarRacing-v0')\n",
    "eval_env = gym.wrappers.resize_observation.ResizeObservation(eval_env, 64)\n",
    "eval_env = gym.wrappers.gray_scale_observation.GrayScaleObservation(eval_env, keep_dim = True)\n",
    "eval_env = sb3.common.vec_env.DummyVecEnv([lambda: eval_env]) \n",
    "eval_env = sb3.common.vec_env.VecFrameStack(eval_env, n_stack=4)\n",
    "\n",
    "# Separate evaluation render env\n",
    "# render_env =  wrappers.Monitor(env, \"./gym-results\", force=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "77o0YzOmQfo2"
   },
   "source": [
    "Create an agent that controls the car using a stack of input image frames as the state input. We recommend a PPO agent with the following hyper-parameters (although you can experiment):\n",
    "- learning_rate = 3e-5\n",
    "- n_steps = 512\n",
    "- ent_coef = 0.001\n",
    "- batch_size = 128\n",
    "- gae_lambda =  0.9\n",
    "- n_epochs = 20\n",
    "- use_sde = True\n",
    "- sde_sample_freq = 4\n",
    "- clip_range = 0.4\n",
    "- policy_kwargs = {'log_std_init': -2, 'ortho_init':False},\n",
    "\n",
    "We also recommend enabling **tensorboard** monitoring of the training process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "id": "4aAxmNANQfo2"
   },
   "outputs": [],
   "source": [
    "tb_log = './tb_logs_StackFrame_Training/'\n",
    "\n",
    "#policy = 'MlpPolicy'\n",
    "policy = 'CnnPolicy'\n",
    "\n",
    "agent = sb3.PPO(policy, env,\n",
    "                    learning_rate = 3e-5,\n",
    "                    n_steps = 512,\n",
    "                    ent_coef = 0.001,\n",
    "                    batch_size = 128,\n",
    "                    gae_lambda =  0.9,\n",
    "                    n_epochs = 20,\n",
    "                    use_sde = True,\n",
    "                    sde_sample_freq = 4,\n",
    "                    clip_range = 0.4,\n",
    "                    policy_kwargs = {'log_std_init': -2, 'ortho_init':False},\n",
    "                    tensorboard_log=tb_log)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IkOXkC-HQfo2"
   },
   "source": [
    "Examine the actor and critic network architectures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "id": "fS-fpmWwQfo3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ActorCriticCnnPolicy(\n",
      "  (features_extractor): NatureCNN(\n",
      "    (cnn): Sequential(\n",
      "      (0): Conv2d(4, 32, kernel_size=(8, 8), stride=(4, 4))\n",
      "      (1): ReLU()\n",
      "      (2): Conv2d(32, 64, kernel_size=(4, 4), stride=(2, 2))\n",
      "      (3): ReLU()\n",
      "      (4): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1))\n",
      "      (5): ReLU()\n",
      "      (6): Flatten(start_dim=1, end_dim=-1)\n",
      "    )\n",
      "    (linear): Sequential(\n",
      "      (0): Linear(in_features=1024, out_features=512, bias=True)\n",
      "      (1): ReLU()\n",
      "    )\n",
      "  )\n",
      "  (mlp_extractor): MlpExtractor(\n",
      "    (shared_net): Sequential()\n",
      "    (policy_net): Sequential()\n",
      "    (value_net): Sequential()\n",
      "  )\n",
      "  (action_net): Linear(in_features=512, out_features=3, bias=True)\n",
      "  (value_net): Linear(in_features=512, out_features=1, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(agent.policy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:blue\">\n",
    "        \n",
    "The difference we can see now comparing the policies is that we are using CNNs (Conv2d) and takes into account the 64x64 dimension of the input images.\n",
    "\n",
    "</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p1jObjvdQfo3"
   },
   "source": [
    "Create an evaluation callback that is called every at regular intervals and renders the episode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "id": "aL2c4LiOQfo4"
   },
   "outputs": [],
   "source": [
    "eval_callback = sb3.common.callbacks.EvalCallback(eval_env, \n",
    "                                                  best_model_save_path='./best_model_CNN_4Stack/',\n",
    "                                                  log_path=tb_log, \n",
    "                                                  eval_freq=5000,\n",
    "                                                  render=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "s33dR-mgQfo4"
   },
   "source": [
    "Train the model for a large number of timesteps (500,000 timesteps will probably work well).\n",
    "\n",
    "<span style=\"color:blue\">\n",
    "        \n",
    "Here we do it only for 10000 timesteps as mentioned before, the result of training it using 500000 timesteps can be seen in the evaluation section.\n",
    "\n",
    "</span>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-629f6fbed82c07cd\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-629f6fbed82c07cd\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          const port = 6006;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%tensorboard --logdir ./tb_logs_StackFrame_Training/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "id": "lRj1j3xjQfo4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Track generation: 1063..1339 -> 276-tiles track\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/david/.local/lib/python3.8/site-packages/stable_baselines3/common/callbacks.py:345: UserWarning: Training and eval env are not of the same type<stable_baselines3.common.vec_env.vec_transpose.VecTransposeImage object at 0x7fae1202ddf0> != <stable_baselines3.common.vec_env.vec_frame_stack.VecFrameStack object at 0x7fae12079ac0>\n",
      "  warnings.warn(\"Training and eval env are not of the same type\" f\"{self.training_env} != {self.eval_env}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Track generation: 1304..1634 -> 330-tiles track\n",
      "Track generation: 1048..1314 -> 266-tiles track\n",
      "Track generation: 1051..1318 -> 267-tiles track\n",
      "Track generation: 1252..1569 -> 317-tiles track\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/david/.local/lib/python3.8/site-packages/stable_baselines3/common/evaluation.py:65: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Track generation: 1081..1355 -> 274-tiles track\n",
      "Track generation: 1136..1424 -> 288-tiles track\n",
      "Track generation: 1112..1394 -> 282-tiles track\n",
      "Track generation: 1265..1585 -> 320-tiles track\n",
      "Track generation: 1262..1585 -> 323-tiles track\n",
      "retry to generate track (normal if there are not manyinstances of this message)\n",
      "Track generation: 1219..1528 -> 309-tiles track\n",
      "Track generation: 1307..1638 -> 331-tiles track\n",
      "Eval num_timesteps=4403, episode_reward=-86.33 +/- 0.79\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Track generation: 1128..1414 -> 286-tiles track\n",
      "Track generation: 1116..1399 -> 283-tiles track\n",
      "Track generation: 1114..1397 -> 283-tiles track\n",
      "Track generation: 1304..1634 -> 330-tiles track\n",
      "Track generation: 1237..1556 -> 319-tiles track\n",
      "Track generation: 1088..1371 -> 283-tiles track\n",
      "Track generation: 1235..1548 -> 313-tiles track\n",
      "Track generation: 1213..1520 -> 307-tiles track\n",
      "Track generation: 1174..1472 -> 298-tiles track\n",
      "Track generation: 1234..1551 -> 317-tiles track\n",
      "Track generation: 1069..1340 -> 271-tiles track\n",
      "Eval num_timesteps=9403, episode_reward=-58.99 +/- 8.40\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Track generation: 1056..1325 -> 269-tiles track\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<stable_baselines3.ppo.ppo.PPO at 0x7fae1202d790>"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent.learn(total_timesteps=10000, callback=eval_callback)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "G8j9GrTTQfo5"
   },
   "source": [
    "Save the trained agent and test its performance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "id": "Z8ivh_VpQfo5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Track generation: 1124..1409 -> 285-tiles track\n",
      "Track generation: 1100..1379 -> 279-tiles track\n",
      "Track generation: 1073..1349 -> 276-tiles track\n",
      "retry to generate track (normal if there are not manyinstances of this message)\n",
      "Track generation: 1520..1904 -> 384-tiles track\n",
      "Track generation: 1140..1429 -> 289-tiles track\n",
      "Track generation: 1015..1278 -> 263-tiles track\n",
      "Track generation: 1177..1475 -> 298-tiles track\n",
      "Track generation: 983..1239 -> 256-tiles track\n",
      "Track generation: 1048..1314 -> 266-tiles track\n",
      "Track generation: 1197..1500 -> 303-tiles track\n",
      "Track generation: 1126..1411 -> 285-tiles track\n",
      "Track generation: 1087..1363 -> 276-tiles track\n",
      "Track generation: 1065..1335 -> 270-tiles track\n",
      "Track generation: 1148..1439 -> 291-tiles track\n",
      "Track generation: 1136..1424 -> 288-tiles track\n",
      "Track generation: 1043..1313 -> 270-tiles track\n",
      "Track generation: 1149..1446 -> 297-tiles track\n",
      "Mean Reward: -26.68511994878451 +/- 41.61278147827246\n"
     ]
    }
   ],
   "source": [
    "# Using MLP policy change to: agent.save(\"./final_models/final_model_CNN_4Stack\")\n",
    "agent.save(\"./final_models/final_model_CNN_4Stack\")\n",
    "\n",
    "mean_reward, std_reward = sb3.common.evaluation.evaluate_policy(agent, \n",
    "                                                                agent.get_env(), \n",
    "                                                                n_eval_episodes=15,\n",
    "                                                                render = True)\n",
    "print(\"Mean Reward: {} +/- {}\".format(mean_reward, std_reward))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For memory management delete old agent and environment (assumes variable names - change if required)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del agent\n",
    "del env\n",
    "del eval_env\n",
    "del render_env"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bWZrzrZ3Qfo5"
   },
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DRzc_-yOQfo5"
   },
   "source": [
    "\n",
    "Now that we have trained and saved multiple agents with different settings, we will proceed to evaluate each of them for 30 episodes and compute the mean reward to test which one yields better results.\n",
    "\n",
    "In particular we have the following four combinations:\n",
    "\n",
    "- MLP with a single image\n",
    "- MLP with 4 stacked images\n",
    "- CNN with a single image\n",
    "- CNN with 4 stacked images\n",
    "\n",
    "We ran these on two different computers for 500000 timestamps.\n",
    "\n",
    "Let's define the possible combinations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "XyfQmsF6Qfo6"
   },
   "outputs": [],
   "source": [
    "policies = ['MLP', 'CNN']\n",
    "images = ['single', '4stack']\n",
    "\n",
    "bestModelsPath = [('./best_modelsC/','PC1'), ('./best_modelsD/','PC2')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Method to load the requested agent and create its corresponding environment: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadAgent(modelPath, policy, images):\n",
    "    modelName = 'best_model_' + policy + '_' + images \n",
    "\n",
    "    if not os.path.isfile(modelPath + modelName + '.zip'):\n",
    "        return -1, -1\n",
    "    \n",
    "    agent = sb3.ppo.PPO.load(modelPath + modelName)\n",
    "    print('Loading', modelPath + modelName)\n",
    "\n",
    "    render_env = gym.make('CarRacing-v0')\n",
    "    render_env = gym.wrappers.resize_observation.ResizeObservation(render_env, 64)\n",
    "    render_env = gym.wrappers.gray_scale_observation.GrayScaleObservation(render_env, keep_dim = True)\n",
    "\n",
    "    if images == '4stack':\n",
    "        render_env = sb3.common.vec_env.DummyVecEnv([lambda: render_env]) \n",
    "        render_env = sb3.common.vec_env.VecFrameStack(render_env, n_stack=4)\n",
    "\n",
    "    # render_env = wrappers.Monitor(render_env, \"./gym-results\", force=True)\n",
    "\n",
    "    agent.set_env(render_env)\n",
    "    return (agent, render_env)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-8X8r5S5Qfo6"
   },
   "source": [
    "Iterate over all possible combinations and execute them for 30 episodes, saving the results in a DataFrame:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "hrMN5vOeQfo6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing single image with MLP policy for ./best_modelsC/\n",
      "Loading ./best_modelsC/best_model_MLP_single\n",
      "Track generation: 1174..1471 -> 297-tiles track\n",
      "Track generation: 1041..1315 -> 274-tiles track\n",
      "Track generation: 1104..1385 -> 281-tiles track\n",
      "Track generation: 1141..1437 -> 296-tiles track\n",
      "Track generation: 1081..1363 -> 282-tiles track\n",
      "Track generation: 1263..1583 -> 320-tiles track\n",
      "Track generation: 1119..1403 -> 284-tiles track\n",
      "Track generation: 1127..1419 -> 292-tiles track\n",
      "Track generation: 1043..1308 -> 265-tiles track\n",
      "Track generation: 1073..1345 -> 272-tiles track\n",
      "Track generation: 1377..1725 -> 348-tiles track\n",
      "Track generation: 1234..1547 -> 313-tiles track\n",
      "Track generation: 1149..1440 -> 291-tiles track\n",
      "Track generation: 1092..1369 -> 277-tiles track\n",
      "Track generation: 1069..1340 -> 271-tiles track\n",
      "Track generation: 1034..1297 -> 263-tiles track\n",
      "Track generation: 1161..1459 -> 298-tiles track\n",
      "Track generation: 1165..1465 -> 300-tiles track\n",
      "Track generation: 1057..1326 -> 269-tiles track\n",
      "Track generation: 1232..1544 -> 312-tiles track\n",
      "Track generation: 1151..1448 -> 297-tiles track\n",
      "Track generation: 1195..1498 -> 303-tiles track\n",
      "Track generation: 1076..1349 -> 273-tiles track\n",
      "Track generation: 1147..1442 -> 295-tiles track\n",
      "Track generation: 1367..1713 -> 346-tiles track\n",
      "Track generation: 1112..1395 -> 283-tiles track\n",
      "Track generation: 1096..1374 -> 278-tiles track\n",
      "Track generation: 1164..1459 -> 295-tiles track\n",
      "Track generation: 1133..1420 -> 287-tiles track\n",
      "Track generation: 1017..1276 -> 259-tiles track\n",
      "Track generation: 1087..1363 -> 276-tiles track\n",
      "Mean Reward: -82.6537795 +/- 1.1953783640207183\n",
      "Testing 4stack image with MLP policy for ./best_modelsC/\n",
      "Loading ./best_modelsC/best_model_MLP_4stack\n",
      "Track generation: 1292..1619 -> 327-tiles track\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/david/.local/lib/python3.8/site-packages/stable_baselines3/common/evaluation.py:65: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Track generation: 1266..1587 -> 321-tiles track\n",
      "Track generation: 1088..1364 -> 276-tiles track\n",
      "Track generation: 1056..1324 -> 268-tiles track\n",
      "Track generation: 1029..1290 -> 261-tiles track\n",
      "Track generation: 1152..1444 -> 292-tiles track\n",
      "Track generation: 1196..1499 -> 303-tiles track\n",
      "Track generation: 1163..1458 -> 295-tiles track\n",
      "Track generation: 1093..1370 -> 277-tiles track\n",
      "Track generation: 1052..1318 -> 266-tiles track\n",
      "Track generation: 1180..1479 -> 299-tiles track\n",
      "Track generation: 1351..1693 -> 342-tiles track\n",
      "Track generation: 1141..1435 -> 294-tiles track\n",
      "Track generation: 1283..1608 -> 325-tiles track\n",
      "Track generation: 1135..1423 -> 288-tiles track\n",
      "Track generation: 1109..1396 -> 287-tiles track\n",
      "Track generation: 1196..1499 -> 303-tiles track\n",
      "Track generation: 1028..1289 -> 261-tiles track\n",
      "Track generation: 977..1233 -> 256-tiles track\n",
      "Track generation: 958..1209 -> 251-tiles track\n",
      "Track generation: 1319..1653 -> 334-tiles track\n",
      "Track generation: 1172..1469 -> 297-tiles track\n",
      "Track generation: 1130..1417 -> 287-tiles track\n",
      "Track generation: 1220..1529 -> 309-tiles track\n",
      "Track generation: 1068..1339 -> 271-tiles track\n",
      "Track generation: 1179..1485 -> 306-tiles track\n",
      "Track generation: 1251..1568 -> 317-tiles track\n",
      "Track generation: 1156..1449 -> 293-tiles track\n",
      "Track generation: 1209..1515 -> 306-tiles track\n",
      "Track generation: 1165..1460 -> 295-tiles track\n",
      "retry to generate track (normal if there are not manyinstances of this message)\n",
      "Track generation: 1152..1444 -> 292-tiles track\n",
      "Track generation: 1245..1560 -> 315-tiles track\n",
      "Mean Reward: 564.2471024185419 +/- 232.2771289045341\n",
      "Testing single image with CNN policy for ./best_modelsC/\n",
      "Not found!\n",
      "Testing 4stack image with CNN policy for ./best_modelsC/\n",
      "Loading ./best_modelsC/best_model_CNN_4stack\n",
      "Track generation: 968..1221 -> 253-tiles track\n",
      "Track generation: 1239..1553 -> 314-tiles track\n",
      "Track generation: 1172..1469 -> 297-tiles track\n",
      "Track generation: 1440..1805 -> 365-tiles track\n",
      "Track generation: 1286..1611 -> 325-tiles track\n",
      "Track generation: 1037..1300 -> 263-tiles track\n",
      "Track generation: 1145..1440 -> 295-tiles track\n",
      "Track generation: 1199..1503 -> 304-tiles track\n",
      "Track generation: 1208..1514 -> 306-tiles track\n",
      "Track generation: 1064..1334 -> 270-tiles track\n",
      "Track generation: 1063..1333 -> 270-tiles track\n",
      "Track generation: 1106..1393 -> 287-tiles track\n",
      "Track generation: 1214..1527 -> 313-tiles track\n",
      "Track generation: 1029..1298 -> 269-tiles track\n",
      "Track generation: 1220..1535 -> 315-tiles track\n",
      "Track generation: 1050..1325 -> 275-tiles track\n",
      "Track generation: 1055..1328 -> 273-tiles track\n",
      "Track generation: 1180..1479 -> 299-tiles track\n",
      "Track generation: 986..1237 -> 251-tiles track\n",
      "Track generation: 1090..1372 -> 282-tiles track\n",
      "Track generation: 1243..1558 -> 315-tiles track\n",
      "Track generation: 1135..1426 -> 291-tiles track\n",
      "retry to generate track (normal if there are not manyinstances of this message)\n",
      "Track generation: 1145..1435 -> 290-tiles track\n",
      "Track generation: 1208..1514 -> 306-tiles track\n",
      "Track generation: 1128..1414 -> 286-tiles track\n",
      "Track generation: 1158..1456 -> 298-tiles track\n",
      "Track generation: 1247..1563 -> 316-tiles track\n",
      "Track generation: 1514..1897 -> 383-tiles track\n",
      "Track generation: 1032..1294 -> 262-tiles track\n",
      "Track generation: 1252..1569 -> 317-tiles track\n",
      "Track generation: 1006..1265 -> 259-tiles track\n",
      "retry to generate track (normal if there are not manyinstances of this message)\n",
      "Track generation: 1066..1340 -> 274-tiles track\n",
      "retry to generate track (normal if there are not manyinstances of this message)\n",
      "Track generation: 1143..1433 -> 290-tiles track\n",
      "Track generation: 1168..1464 -> 296-tiles track\n",
      "Mean Reward: 675.4497284678122 +/- 208.86173568520772\n",
      "Testing single image with MLP policy for ./best_modelsD/\n",
      "Loading ./best_modelsD/best_model_MLP_single\n",
      "Track generation: 1189..1494 -> 305-tiles track\n",
      "retry to generate track (normal if there are not manyinstances of this message)\n",
      "Track generation: 1052..1323 -> 271-tiles track\n",
      "Track generation: 1074..1375 -> 301-tiles track\n",
      "retry to generate track (normal if there are not manyinstances of this message)\n",
      "Track generation: 1216..1524 -> 308-tiles track\n",
      "Track generation: 1060..1337 -> 277-tiles track\n",
      "Track generation: 1028..1289 -> 261-tiles track\n",
      "Track generation: 1015..1273 -> 258-tiles track\n",
      "Track generation: 1182..1488 -> 306-tiles track\n",
      "Track generation: 1141..1430 -> 289-tiles track\n",
      "Track generation: 1033..1304 -> 271-tiles track\n",
      "Track generation: 1292..1619 -> 327-tiles track\n",
      "Track generation: 1041..1314 -> 273-tiles track\n",
      "Track generation: 1028..1293 -> 265-tiles track\n",
      "Track generation: 1108..1389 -> 281-tiles track\n",
      "Track generation: 1213..1520 -> 307-tiles track\n",
      "Track generation: 1166..1461 -> 295-tiles track\n",
      "Track generation: 911..1150 -> 239-tiles track\n",
      "Track generation: 1206..1512 -> 306-tiles track\n",
      "Track generation: 1112..1394 -> 282-tiles track\n",
      "Track generation: 1160..1454 -> 294-tiles track\n",
      "Track generation: 1112..1400 -> 288-tiles track\n",
      "Track generation: 1116..1399 -> 283-tiles track\n",
      "Track generation: 1159..1453 -> 294-tiles track\n",
      "Track generation: 1208..1514 -> 306-tiles track\n",
      "Track generation: 893..1124 -> 231-tiles track\n",
      "retry to generate track (normal if there are not manyinstances of this message)\n",
      "Track generation: 1246..1561 -> 315-tiles track\n",
      "Track generation: 1165..1460 -> 295-tiles track\n",
      "Track generation: 1146..1436 -> 290-tiles track\n",
      "Track generation: 1123..1412 -> 289-tiles track\n",
      "Track generation: 1400..1753 -> 353-tiles track\n",
      "Track generation: 1178..1478 -> 300-tiles track\n",
      "retry to generate track (normal if there are not manyinstances of this message)\n",
      "Track generation: 1196..1505 -> 309-tiles track\n",
      "Track generation: 1130..1417 -> 287-tiles track\n",
      "Track generation: 1159..1453 -> 294-tiles track\n",
      "Track generation: 1140..1429 -> 289-tiles track\n",
      "Mean Reward: 226.8588700666667 +/- 125.15683922580274\n",
      "Testing 4stack image with MLP policy for ./best_modelsD/\n",
      "Loading ./best_modelsD/best_model_MLP_4stack\n",
      "Track generation: 1132..1419 -> 287-tiles track\n",
      "Track generation: 1080..1354 -> 274-tiles track\n",
      "Track generation: 1086..1364 -> 278-tiles track\n",
      "retry to generate track (normal if there are not manyinstances of this message)\n",
      "Track generation: 1132..1426 -> 294-tiles track\n",
      "Track generation: 1059..1328 -> 269-tiles track\n",
      "Track generation: 1186..1487 -> 301-tiles track\n",
      "Track generation: 1115..1405 -> 290-tiles track\n",
      "Track generation: 1112..1394 -> 282-tiles track\n",
      "Track generation: 1174..1471 -> 297-tiles track\n",
      "Track generation: 1161..1457 -> 296-tiles track\n",
      "retry to generate track (normal if there are not manyinstances of this message)\n",
      "Track generation: 1073..1350 -> 277-tiles track\n",
      "Track generation: 1108..1389 -> 281-tiles track\n",
      "Track generation: 1204..1509 -> 305-tiles track\n",
      "Track generation: 1183..1483 -> 300-tiles track\n",
      "Track generation: 1017..1278 -> 261-tiles track\n",
      "retry to generate track (normal if there are not manyinstances of this message)\n",
      "Track generation: 1203..1508 -> 305-tiles track\n",
      "Track generation: 1096..1374 -> 278-tiles track\n",
      "Track generation: 992..1244 -> 252-tiles track\n",
      "Track generation: 1161..1464 -> 303-tiles track\n",
      "Track generation: 1039..1303 -> 264-tiles track\n",
      "Track generation: 1119..1407 -> 288-tiles track\n",
      "Track generation: 1404..1765 -> 361-tiles track\n",
      "Track generation: 1156..1457 -> 301-tiles track\n",
      "Track generation: 1060..1329 -> 269-tiles track\n",
      "Track generation: 1148..1439 -> 291-tiles track\n",
      "Track generation: 1108..1389 -> 281-tiles track\n",
      "Track generation: 1179..1478 -> 299-tiles track\n",
      "Track generation: 1102..1387 -> 285-tiles track\n",
      "Track generation: 1102..1391 -> 289-tiles track\n",
      "Track generation: 1231..1543 -> 312-tiles track\n",
      "Track generation: 1268..1589 -> 321-tiles track\n",
      "Track generation: 1087..1363 -> 276-tiles track\n",
      "Track generation: 1012..1269 -> 257-tiles track\n",
      "Track generation: 1255..1572 -> 317-tiles track\n",
      "Mean Reward: 611.9193124348918 +/- 221.35844744404605\n",
      "Testing single image with CNN policy for ./best_modelsD/\n",
      "Loading ./best_modelsD/best_model_CNN_single\n",
      "Track generation: 1195..1505 -> 310-tiles track\n",
      "Track generation: 1156..1449 -> 293-tiles track\n",
      "Track generation: 1283..1608 -> 325-tiles track\n",
      "Track generation: 995..1253 -> 258-tiles track\n",
      "Track generation: 1160..1454 -> 294-tiles track\n",
      "Track generation: 1048..1314 -> 266-tiles track\n",
      "Track generation: 1104..1384 -> 280-tiles track\n",
      "Track generation: 1227..1538 -> 311-tiles track\n",
      "Track generation: 1165..1461 -> 296-tiles track\n",
      "Track generation: 1283..1607 -> 324-tiles track\n",
      "Track generation: 1163..1458 -> 295-tiles track\n",
      "Track generation: 1069..1350 -> 281-tiles track\n",
      "Track generation: 987..1241 -> 254-tiles track\n",
      "retry to generate track (normal if there are not manyinstances of this message)\n",
      "Track generation: 1138..1431 -> 293-tiles track\n",
      "Track generation: 1136..1424 -> 288-tiles track\n",
      "Track generation: 1104..1384 -> 280-tiles track\n",
      "Track generation: 1071..1343 -> 272-tiles track\n",
      "Track generation: 1316..1649 -> 333-tiles track\n",
      "Track generation: 1049..1315 -> 266-tiles track\n",
      "Track generation: 1157..1450 -> 293-tiles track\n",
      "Track generation: 1167..1463 -> 296-tiles track\n",
      "Track generation: 1346..1687 -> 341-tiles track\n",
      "Track generation: 1135..1423 -> 288-tiles track\n",
      "Track generation: 1132..1419 -> 287-tiles track\n",
      "Track generation: 1180..1484 -> 304-tiles track\n",
      "Track generation: 1008..1264 -> 256-tiles track\n",
      "Track generation: 990..1241 -> 251-tiles track\n",
      "Track generation: 1157..1451 -> 294-tiles track\n",
      "Track generation: 1242..1557 -> 315-tiles track\n",
      "Track generation: 1112..1394 -> 282-tiles track\n",
      "Track generation: 1128..1421 -> 293-tiles track\n",
      "Track generation: 1252..1569 -> 317-tiles track\n",
      "Mean Reward: 320.68639376666664 +/- 116.10888002845985\n",
      "Testing 4stack image with CNN policy for ./best_modelsD/\n",
      "Loading ./best_modelsD/best_model_CNN_4stack\n",
      "Track generation: 1275..1598 -> 323-tiles track\n",
      "Track generation: 1071..1343 -> 272-tiles track\n",
      "Track generation: 1150..1442 -> 292-tiles track\n",
      "Track generation: 1217..1531 -> 314-tiles track\n",
      "Track generation: 1119..1404 -> 285-tiles track\n",
      "Track generation: 1002..1264 -> 262-tiles track\n",
      "Track generation: 1236..1557 -> 321-tiles track\n",
      "Track generation: 950..1199 -> 249-tiles track\n",
      "Track generation: 1068..1339 -> 271-tiles track\n",
      "Track generation: 1442..1806 -> 364-tiles track\n",
      "Track generation: 1217..1525 -> 308-tiles track\n",
      "Track generation: 1102..1381 -> 279-tiles track\n",
      "Track generation: 1104..1384 -> 280-tiles track\n",
      "Track generation: 1172..1469 -> 297-tiles track\n",
      "Track generation: 1162..1466 -> 304-tiles track\n",
      "Track generation: 1160..1454 -> 294-tiles track\n",
      "Track generation: 1241..1555 -> 314-tiles track\n",
      "Track generation: 1216..1524 -> 308-tiles track\n",
      "Track generation: 1044..1309 -> 265-tiles track\n",
      "Track generation: 1161..1463 -> 302-tiles track\n",
      "Track generation: 1046..1322 -> 276-tiles track\n",
      "Track generation: 1133..1429 -> 296-tiles track\n",
      "Track generation: 961..1206 -> 245-tiles track\n",
      "Track generation: 1159..1460 -> 301-tiles track\n",
      "Track generation: 1149..1440 -> 291-tiles track\n",
      "Track generation: 1061..1331 -> 270-tiles track\n",
      "Track generation: 1403..1758 -> 355-tiles track\n",
      "Track generation: 1217..1527 -> 310-tiles track\n",
      "retry to generate track (normal if there are not manyinstances of this message)\n",
      "Track generation: 1036..1299 -> 263-tiles track\n",
      "Track generation: 1200..1504 -> 304-tiles track\n",
      "Track generation: 1154..1447 -> 293-tiles track\n",
      "Track generation: 1167..1463 -> 296-tiles track\n",
      "Mean Reward: 449.6268477054934 +/- 59.669725756878826\n"
     ]
    }
   ],
   "source": [
    "results = {'MLP_single_PC1':{}, 'MLP_4stack_PC1':{}, 'CNN_single_PC1':{}, 'CNN_4stack_PC1':{},\n",
    "           'MLP_single_PC2':{}, 'MLP_4stack_PC2':{}, 'CNN_single_PC2':{}, 'CNN_4stack_PC2':{}}\n",
    "\n",
    "for modelPath, pathId in bestModelsPath:\n",
    "    for policy in policies:\n",
    "        for img in images:\n",
    "            model_id = policy + '_' + img + '_' + pathId\n",
    "            print(\"Testing\", img, \"image with\", policy, \"policy\", \"for\", modelPath)\n",
    "            agent, env = loadAgent(modelPath, policy, img)\n",
    "            \n",
    "            if agent == -1:\n",
    "                print(\"Not found!\")\n",
    "                results[model_id]['Mean'] = -1\n",
    "                results[model_id]['std'] = -1\n",
    "                continue\n",
    "\n",
    "            mean_reward, std_reward = sb3.common.evaluation.evaluate_policy(agent, \n",
    "                                                                            agent.get_env(), \n",
    "                                                                            n_eval_episodes=30,\n",
    "                                                                            render = True)\n",
    "            print(\"Mean Reward: {} +/- {}\".format(mean_reward, std_reward))\n",
    "            \n",
    "            results[model_id]['Mean'] = mean_reward\n",
    "            results[model_id]['std'] = std_reward\n",
    "            del agent\n",
    "            del env\n",
    "\n",
    "resultsDF = pd.DataFrame(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And these are the results we obtain after 30 episodes for each possibility:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      MLP_single_PC1  MLP_4stack_PC1  CNN_single_PC1  CNN_4stack_PC1  MLP_single_PC2  MLP_4stack_PC2  CNN_single_PC2  CNN_4stack_PC2\n",
      "Mean      -82.653779      564.247102              -1      675.449728      226.858870      611.919312      320.686394      449.626848\n",
      "std         1.195378      232.277129              -1      208.861736      125.156839      221.358447      116.108880       59.669726\n"
     ]
    }
   ],
   "source": [
    "print(resultsDF)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reflection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WRCxNiPqQfo7"
   },
   "source": [
    "Reflect on which  agent performs better at the task, and the training process involved (max 200 words)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelPath = \"./best_modelsC\"\n",
    "modelPath = \"CNN\"\n",
    "img = \"4Stack\"\n",
    "agent, env = loadAgent(modelPath, policy, img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- if we run sometimes 800 well\n",
    "\n",
    "best are MLP_4stackpc1, CNN4stackPC1, MLPstackPC2 and CNN4stackPC2\n",
    "\n",
    "load these \n",
    "\n",
    "Often times the best model reached before 500k, then it "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multiple environments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another approach that we tried to test as well was using multiple parallel environments for training to take advantage of the multi-processing power of the computers.\n",
    "\n",
    "The idea here is that using SubprocVecEnv we create a vectorized wrapper that will run different environments each step, which can help with the performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because we couldn't get to work make_vec_env directly if we wanted the gray_scale and resize wrappers as well, we performed the vectorization manually, mofidying the code taken from the sb3 documentation: https://colab.research.google.com/github/araffin/rl-tutorial-jnrr19/blob/sb3/3_multiprocessing.ipynb#scrollTo=AvO5BGrVv2Rk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Callable\n",
    "from stable_baselines3.common.utils import set_random_seed\n",
    "\n",
    "def make_env(env_id: str, rank: int, seed: int = 0) -> Callable:\n",
    "    def _init() -> gym.Env:\n",
    "        env = gym.make(env_id)\n",
    "        env = gym.wrappers.resize_observation.ResizeObservation(env, 64)\n",
    "        env = gym.wrappers.gray_scale_observation.GrayScaleObservation(env, keep_dim = True)\n",
    "        env.seed(seed + rank)\n",
    "\n",
    "        return env\n",
    "    set_random_seed(seed)\n",
    "    return _init\n",
    "\n",
    "env_id = \"CarRacing-v0\"\n",
    "num_cpu = 4\n",
    "\n",
    "# Create the vectorized environment\n",
    "multi_env = sb3.common.vec_env.SubprocVecEnv([make_env(env_id, i) for i in range(num_cpu)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "multi_eval_env = sb3.common.vec_env.SubprocVecEnv([make_env(env_id, i) for i in range(num_cpu)])\n",
    "tb_log = \"tb_test\"\n",
    "\n",
    "eval_callback = sb3.common.callbacks.EvalCallback(multi_eval_env, \n",
    "                                                  best_model_save_path='./best_model_multi/',\n",
    "                                                  log_path=tb_log, \n",
    "                                                  eval_freq=2000,\n",
    "                                                  render=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "tb_log = \"tb_test\"\n",
    "agent = sb3.PPO('CnnPolicy', multi_env,\n",
    "                    learning_rate = 3e-5,\n",
    "                    n_steps = 512,\n",
    "                    ent_coef = 0.001,\n",
    "                    batch_size = 128,\n",
    "                    gae_lambda =  0.9,\n",
    "                    n_epochs = 20,\n",
    "                    use_sde = True,\n",
    "                    sde_sample_freq = 4,\n",
    "                    clip_range = 0.4,\n",
    "                    policy_kwargs = {'log_std_init': -2, 'ortho_init':False},\n",
    "                    tensorboard_log=tb_log)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we didn't train it up to 500000 but we just wanted to test this as another possible option for our system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Track generation: 1056..1324 -> 268-tiles trackTrack generation: 1108..1389 -> 281-tiles track\n",
      "\n",
      "Track generation: 1055..1332 -> 277-tiles track\n",
      "Track generation: 1143..1442 -> 299-tiles track\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/david/.local/lib/python3.8/site-packages/stable_baselines3/common/callbacks.py:345: UserWarning: Training and eval env are not of the same type<stable_baselines3.common.vec_env.vec_transpose.VecTransposeImage object at 0x7fae127458e0> != <stable_baselines3.common.vec_env.subproc_vec_env.SubprocVecEnv object at 0x7fae12079af0>\n",
      "  warnings.warn(\"Training and eval env are not of the same type\" f\"{self.training_env} != {self.eval_env}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Track generation: 1231..1543 -> 312-tiles track\n",
      "Track generation: 1084..1359 -> 275-tiles track\n",
      "Track generation: 1069..1347 -> 278-tiles track\n",
      "Track generation: 1087..1369 -> 282-tiles track\n",
      "Track generation: 1199..1503 -> 304-tiles track\n",
      "Track generation: 981..1234 -> 253-tiles track\n",
      "retry to generate track (normal if there are not manyinstances of this message)\n",
      "Track generation: 1251..1568 -> 317-tiles track\n",
      "Track generation: 964..1212 -> 248-tiles track\n",
      "retry to generate track (normal if there are not manyinstances of this message)\n",
      "Track generation: 1176..1474 -> 298-tiles track\n",
      "Track generation: 1055..1323 -> 268-tiles track\n",
      "Track generation: 1055..1332 -> 277-tiles track\n",
      "Track generation: 1108..1389 -> 281-tiles track\n",
      "Track generation: 1143..1442 -> 299-tiles track\n",
      "Track generation: 1056..1324 -> 268-tiles track\n",
      "Track generation: 1069..1347 -> 278-tiles track\n",
      "Track generation: 1084..1359 -> 275-tiles track\n",
      "Track generation: 1087..1369 -> 282-tiles track\n",
      "Track generation: 1231..1543 -> 312-tiles track\n",
      "Track generation: 981..1234 -> 253-tiles track\n",
      "retry to generate track (normal if there are not manyinstances of this message)\n",
      "Track generation: 1251..1568 -> 317-tiles track\n",
      "Track generation: 964..1212 -> 248-tiles track\n",
      "Track generation: 1199..1503 -> 304-tiles track\n",
      "retry to generate track (normal if there are not manyinstances of this message)\n",
      "Track generation: 1055..1323 -> 268-tiles track\n",
      "Track generation: 1176..1474 -> 298-tiles track\n",
      "Eval num_timesteps=8000, episode_reward=-64.11 +/- 1.30\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Track generation: 1232..1544 -> 312-tiles track\n",
      "Track generation: 1113..1395 -> 282-tiles track\n",
      "Track generation: 1187..1488 -> 301-tiles track\n",
      "Track generation: 1283..1608 -> 325-tiles track\n",
      "Track generation: 1032..1301 -> 269-tiles track\n",
      "Track generation: 1264..1584 -> 320-tiles track\n",
      "Track generation: 1217..1526 -> 309-tiles track\n",
      "Track generation: 1103..1383 -> 280-tiles track\n",
      "Track generation: 1283..1608 -> 325-tiles track\n",
      "Track generation: 1232..1544 -> 312-tiles track\n",
      "Track generation: 1113..1395 -> 282-tiles track\n",
      "Track generation: 1187..1488 -> 301-tiles track\n",
      "Track generation: 1264..1584 -> 320-tiles track\n",
      "Track generation: 1032..1301 -> 269-tiles track\n",
      "Track generation: 1103..1383 -> 280-tiles track\n",
      "Track generation: 1217..1526 -> 309-tiles track\n",
      "Track generation: 1096..1374 -> 278-tiles track\n",
      "Track generation: 1008..1264 -> 256-tiles track\n",
      "Track generation: 1152..1444 -> 292-tiles track\n",
      "Track generation: 1131..1418 -> 287-tiles track\n",
      "Eval num_timesteps=16000, episode_reward=-53.02 +/- 2.70\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Track generation: 1152..1444 -> 292-tiles track\n",
      "Track generation: 1131..1418 -> 287-tiles track\n",
      "Track generation: 1096..1374 -> 278-tiles track\n",
      "Track generation: 1008..1264 -> 256-tiles track\n",
      "Track generation: 983..1241 -> 258-tiles track\n",
      "Track generation: 1152..1444 -> 292-tiles track\n",
      "Track generation: 1091..1368 -> 277-tiles track\n",
      "Track generation: 1198..1501 -> 303-tiles track\n",
      "Track generation: 1152..1444 -> 292-tiles track\n",
      "Track generation: 1198..1501 -> 303-tiles track\n",
      "Track generation: 983..1241 -> 258-tiles track\n",
      "Track generation: 1091..1368 -> 277-tiles track\n",
      "Track generation: 1172..1469 -> 297-tiles track\n",
      "Track generation: 1159..1453 -> 294-tiles track\n",
      "Track generation: 1077..1359 -> 282-tiles track\n",
      "Track generation: 1106..1388 -> 282-tiles track\n",
      "retry to generate track (normal if there are not manyinstances of this message)\n",
      "Track generation: 1283..1608 -> 325-tiles track\n",
      "Track generation: 1188..1489 -> 301-tiles track\n",
      "Track generation: 957..1205 -> 248-tiles track\n",
      "Track generation: 1136..1424 -> 288-tiles track\n",
      "Track generation: 1090..1367 -> 277-tiles track\n",
      "Eval num_timesteps=24000, episode_reward=-30.25 +/- 19.40\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Track generation: 1106..1388 -> 282-tiles track\n",
      "retry to generate track (normal if there are not manyinstances of this message)\n",
      "Track generation: 1283..1608 -> 325-tiles track\n",
      "Track generation: 1172..1469 -> 297-tiles track\n",
      "Track generation: 1077..1359 -> 282-tiles track\n",
      "Track generation: 1159..1453 -> 294-tiles track\n",
      "Track generation: 1188..1489 -> 301-tiles track\n",
      "Track generation: 1136..1424 -> 288-tiles track\n",
      "Track generation: 1090..1367 -> 277-tiles track\n",
      "Track generation: 957..1205 -> 248-tiles track\n",
      "Track generation: 1181..1480 -> 299-tiles track\n",
      "Track generation: 1286..1612 -> 326-tiles track\n",
      "Track generation: 1060..1329 -> 269-tiles track\n",
      "Track generation: 1231..1543 -> 312-tiles track\n",
      "Track generation: 1064..1334 -> 270-tiles track\n",
      "Track generation: 1207..1513 -> 306-tiles track\n",
      "Track generation: 1166..1463 -> 297-tiles track\n",
      "Track generation: 979..1234 -> 255-tiles track\n",
      "Track generation: 1172..1469 -> 297-tiles track\n",
      "Track generation: 985..1244 -> 259-tiles track\n",
      "Track generation: 1320..1654 -> 334-tiles track\n",
      "Track generation: 1083..1365 -> 282-tiles track\n",
      "Eval num_timesteps=32000, episode_reward=-59.06 +/- 13.56\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "Track generation: 1231..1543 -> 312-tiles track\n",
      "Track generation: 1181..1480 -> 299-tiles track\n",
      "Track generation: 1060..1329 -> 269-tiles track\n",
      "Track generation: 1286..1612 -> 326-tiles track\n",
      "Track generation: 1064..1334 -> 270-tiles track\n",
      "Track generation: 979..1234 -> 255-tiles track\n",
      "Track generation: 1207..1513 -> 306-tiles track\n",
      "Track generation: 1166..1463 -> 297-tiles track\n",
      "Track generation: 943..1191 -> 248-tiles track\n",
      "Track generation: 1067..1338 -> 271-tiles track\n",
      "Track generation: 1264..1584 -> 320-tiles track\n",
      "Track generation: 1236..1549 -> 313-tiles track\n",
      "Track generation: 1067..1338 -> 271-tiles track\n",
      "Track generation: 1208..1514 -> 306-tiles track\n",
      "Track generation: 1079..1353 -> 274-tiles track\n",
      "Track generation: 1072..1350 -> 278-tiles track\n",
      "Track generation: 1207..1513 -> 306-tiles track\n",
      "Track generation: 1154..1452 -> 298-tiles trackTrack generation: 1187..1488 -> 301-tiles track\n",
      "\n",
      "Track generation: 1055..1324 -> 269-tiles track\n",
      "Eval num_timesteps=40000, episode_reward=289.65 +/- 133.50\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Track generation: 985..1244 -> 259-tiles track\n",
      "Track generation: 1083..1365 -> 282-tiles track\n",
      "Track generation: 1320..1654 -> 334-tiles track\n",
      "Track generation: 1172..1469 -> 297-tiles track\n",
      "Track generation: 1236..1549 -> 313-tiles track\n",
      "Track generation: 943..1191 -> 248-tiles track\n",
      "Track generation: 1067..1338 -> 271-tiles track\n",
      "Track generation: 1264..1584 -> 320-tiles track\n",
      "Track generation: 1049..1325 -> 276-tiles track\n",
      "Track generation: 1228..1539 -> 311-tiles track\n",
      "Track generation: 1126..1416 -> 290-tiles track\n",
      "Track generation: 1106..1396 -> 290-tiles track\n",
      "Track generation: 963..1208 -> 245-tiles track\n",
      "Track generation: 1044..1304 -> 260-tiles track\n",
      "retry to generate track (normal if there are not manyinstances of this message)\n",
      "Track generation: 1156..1455 -> 299-tiles track\n",
      "Track generation: 1296..1628 -> 332-tiles track\n",
      "retry to generate track (normal if there are not manyinstances of this message)\n",
      "Track generation: 1160..1459 -> 299-tiles track\n",
      "Track generation: 1047..1319 -> 272-tiles track\n",
      "Track generation: 1116..1399 -> 283-tiles track\n",
      "Track generation: 1324..1659 -> 335-tiles track\n",
      "Track generation: 1140..1429 -> 289-tiles track\n",
      "Track generation: 1245..1560 -> 315-tiles track\n",
      "Eval num_timesteps=48000, episode_reward=-49.65 +/- 14.69\n",
      "Episode length: 1000.00 +/- 0.00\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<stable_baselines3.ppo.ppo.PPO at 0x7fae12097f70>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent.learn(total_timesteps=100000, callback=eval_callback)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the agent for future use:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.save(\"./best_model_multi/best_model_multi\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we evaluate it as well we can see that the displayed episodes show the four environments running at the same time:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Track generation: 1079..1353 -> 274-tiles track\n",
      "Track generation: 1208..1514 -> 306-tiles trackTrack generation: 1072..1350 -> 278-tiles track\n",
      "\n",
      "Track generation: 1067..1338 -> 271-tiles track\n",
      "Track generation: 1207..1513 -> 306-tiles track\n",
      "Track generation: 1154..1452 -> 298-tiles track\n",
      "Track generation: 1055..1324 -> 269-tiles track\n",
      "Track generation: 1187..1488 -> 301-tiles track\n",
      "Track generation: 1228..1539 -> 311-tiles track\n",
      "Track generation: 1126..1416 -> 290-tiles track\n",
      "Track generation: 1106..1396 -> 290-tiles track\n",
      "Track generation: 1049..1325 -> 276-tiles track\n",
      "Track generation: 963..1208 -> 245-tiles track\n",
      "Track generation: 1156..1455 -> 299-tiles track\n",
      "Track generation: 1324..1659 -> 335-tiles track\n",
      "Track generation: 1296..1628 -> 332-tiles track\n",
      "retry to generate track (normal if there are not manyinstances of this message)\n",
      "Track generation: 1047..1319 -> 272-tiles track\n",
      "Track generation: 1044..1304 -> 260-tiles track\n",
      "retry to generate track (normal if there are not manyinstances of this message)\n",
      "Track generation: 1160..1459 -> 299-tiles track\n",
      "Track generation: 1034..1297 -> 263-tiles track\n",
      "Track generation: 1116..1399 -> 283-tiles track\n",
      "Track generation: 1245..1560 -> 315-tiles track\n",
      "Track generation: 1140..1429 -> 289-tiles track\n",
      "Mean Reward: 28.84181324248703 +/- 88.70107322990523\n"
     ]
    }
   ],
   "source": [
    "mean_reward, std_reward = sb3.common.evaluation.evaluate_policy(agent, \n",
    "                                                                agent.get_env(), \n",
    "                                                                n_eval_episodes=15,\n",
    "                                                                render = True)\n",
    "print(\"Mean Reward: {} +/- {}\".format(mean_reward, std_reward))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's hard to say whether it would perform better, but it seems to train significantly faster than the other approaches."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "machine_shape": "hm",
   "name": "WS3 Going The Distance.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "164.571px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
