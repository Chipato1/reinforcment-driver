{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5eQU7pJxQfod"
   },
   "source": [
    "# COMP47590 Advanced Machine Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rPgBaQLxQfoj"
   },
   "source": [
    "## Assignment 2: Going the Distance\n",
    "Uses the PPO actor-critic method to train a neural network to control a simple robot in the RacingCar environment from OpenAI gym (https://gym.openai.com/envs/RacingCar-v0/). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NZNiGMqsQfok"
   },
   "source": [
    "![Racing](racing_car.gif)\n",
    "\n",
    "There are five discrete **actions** in this environment:\n",
    "- left (0)\n",
    "- right (1)\n",
    "- brake (2)\n",
    "- accelerate (3)\n",
    "- none (4)\n",
    "\n",
    "**Reward** of -0.1 is awarded every frame and +1000/N for every track tile visited, where N is the total number of tiles in track. For example, if you have finished in 732 frames, your reward is 1000 - 0.1*732 = 926.8 points.\n",
    "\n",
    "And the **state** is represented using a single image frame (96 * 96)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:blue\">\n",
    "    \n",
    "## Foreword: How we conducted the work for this assingment\n",
    "    \n",
    "- Use monitor to show it in colab and save mp4 to share\n",
    "- Work on different machines - Exchange ZIP files\n",
    "- We will highlight our texts in this notebook in **blue**\n",
    "\n",
    "</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Dm_TwuoRQfol"
   },
   "source": [
    "### Initialisation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VHWFxiANQfom"
   },
   "source": [
    "If using Google colab you need to install packages - comment out lines below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0XEhFeXkQfom",
    "outputId": "42e5ffd2-1cd3-4daf-b535-82e7c00c2dde"
   },
   "outputs": [],
   "source": [
    "## We used this in the colab environment\n",
    "\n",
    "#!apt install swig cmake ffmpeg\n",
    "#!apt-get install -y xvfb x11-utils\n",
    "#!pip install stable-baselines3[extra] pyglet box2d box2d-kengz\n",
    "#!pip install pyvirtualdisplay PyOpenGL PyOpenGL-accelerate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mV9IM5FRQfoo"
   },
   "source": [
    "For Google colab comment out this cell to make a virtual rendering canvas so render calls work (we still won't see display!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zh2cw7iSQfop"
   },
   "outputs": [],
   "source": [
    "#import pyvirtualdisplay\n",
    "#\n",
    "#_display = pyvirtualdisplay.Display(visible=False,  # use False with Xvfb\n",
    "#                                    size=(1400, 900))\n",
    "#_ = _display.start()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "S8Bcx9GkQfoq"
   },
   "source": [
    "Import required packages. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dZcWCINuQfor"
   },
   "outputs": [],
   "source": [
    "import torch \n",
    "import gym\n",
    "import stable_baselines3 as sb3\n",
    "\n",
    "import pandas as pd # For data frames and data frame manipulation\n",
    "pd.set_option('display.max_rows', 100)\n",
    "pd.set_option('display.max_columns', 100)\n",
    "pd.set_option('display.width', 1000)\n",
    "import numpy as np # For general  numeric operations\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports for MP4 rendering\n",
    "import io\n",
    "import base64\n",
    "from IPython.display import HTML\n",
    "from gym import wrappers\n",
    "\n",
    "# Imports for inline Tensorboard\n",
    "%load_ext tensorboard\n",
    "import datetime, os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In colab we ensure to use the GPU for training\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Here comes all the logic for showing the mp4s\n",
    "\n",
    "def show_render_result(rend_env):\n",
    "  video = io.open('./gym-results/openaigym.video.%s.video000000.mp4' % rend_env.file_infix, 'r+b').read()\n",
    "  encoded = base64.b64encode(video)\n",
    "  return HTML(data=''' \n",
    "  <video width=\"720\" height=\"auto\" alt=\"test\" controls><source src=\"data:video/mp4;base64,{0}\" type=\"video/mp4\" /></video>'''\n",
    "  .format(encoded.decode('ascii')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GEJsAysfQfos"
   },
   "source": [
    "### Create and Explore the Environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CCMqLB1VQfot"
   },
   "source": [
    "Create the **CarRacing-v0** environment. Add wrappers to resize the images and convert to greyscale."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "w7xkqRzcQfot",
    "outputId": "4a53b8ef-2971-4743-96b6-5172c8693737"
   },
   "outputs": [],
   "source": [
    "env = gym.make('CarRacing-v0')\n",
    "env = gym.wrappers.resize_observation.ResizeObservation(env, 64)\n",
    "env = gym.wrappers.gray_scale_observation.GrayScaleObservation(env, keep_dim = True)\n",
    "\n",
    "# This is the env we use to monitor when we want a video of the agent\n",
    "render_env =  wrappers.Monitor(env, \"./gym-results\", force=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "F9BQTzRKQfou"
   },
   "source": [
    "Explore the environment - view the action space and observation space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "peo_ntRkQfou",
    "outputId": "e9170d3a-93e5-4305-89b8-6719ef0f4bde"
   },
   "outputs": [],
   "source": [
    "print(\"action_space: \", env.action_space)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZgObNy4uQfov",
    "outputId": "f319f83d-c61b-47e8-ccac-99e5b39fe4f5"
   },
   "outputs": [],
   "source": [
    "print(\"env.observation_space\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t-QsvRe0Qfow"
   },
   "source": [
    "Play an episode of the environment using random actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "f_jvIhrjQfox",
    "outputId": "f12cbfa7-23a2-4efd-b58f-4729be5eb558"
   },
   "outputs": [],
   "source": [
    "obs = env.reset()\n",
    "done = False\n",
    "\n",
    "while not done:\n",
    "    action = env.action_space.sample()\n",
    "    obs, reward, done, info = env.step(action)\n",
    "    env.render()\n",
    "    env.render('rgb_array')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:blue\">\n",
    "    \n",
    "## Exploration of the Environment\n",
    "    \n",
    "- Write about shapes and wrappers\n",
    "\n",
    "</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2ml3JnJ5Qfox"
   },
   "source": [
    "###Â Single Image Agent\n",
    "Create an agent that controls the car using a single image frame as the state input. We recommend a PPO agent with the following hyper-parameters (although you can experiment):\n",
    "- learning_rate = 3e-5\n",
    "- n_steps = 512\n",
    "- ent_coef = 0.001\n",
    "- batch_size = 128\n",
    "- gae_lambda =  0.9\n",
    "- n_epochs = 20\n",
    "- use_sde = True\n",
    "- sde_sample_freq = 4\n",
    "- clip_range = 0.4\n",
    "- policy_kwargs = {'log_std_init': -2, 'ortho_init':False},\n",
    "\n",
    "We also recommend enabling **tensorboard** monitoring of the training process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GHY19WhjQfoy",
    "outputId": "38bcf412-18ea-40db-b6b5-b1bff5550f22"
   },
   "outputs": [],
   "source": [
    "tb_log = './tb_logs_SingleFrame_Training/'\n",
    "\n",
    "#policy = 'MlpPolicy'\n",
    "policy = 'CnnPolicy'\n",
    "\n",
    "agent = sb3.PPO(policy, env,\n",
    "                    learning_rate = 3e-5,\n",
    "                    n_steps = 512,\n",
    "                    ent_coef = 0.001,\n",
    "                    batch_size = 128,\n",
    "                    gae_lambda =  0.9,\n",
    "                    n_epochs = 20,\n",
    "                    use_sde = True,\n",
    "                    sde_sample_freq = 4,\n",
    "                    clip_range = 0.4,\n",
    "                    policy_kwargs = {'log_std_init': -2, 'ortho_init':False},\n",
    "                    tensorboard_log=tb_log)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aaVvGXkNQfoz"
   },
   "source": [
    "Examine the actor and critic network architectures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mNjosaimQfoz",
    "outputId": "c64af8eb-3cb9-4d18-f1cd-c3251ea3904a"
   },
   "outputs": [],
   "source": [
    "print(agent.policy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:blue\">\n",
    "    \n",
    "## On the actor and critic network\n",
    "    \n",
    "- 2 heads, CNN or MLP output dfims\n",
    "\n",
    "</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pdRpUwTeQfoz"
   },
   "source": [
    "Create an evaluation callback that is called every at regular intervals and renders the episode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "oulh4r78Qfoz",
    "outputId": "876d8029-1ce4-46b7-e338-6643dadb8dd9"
   },
   "outputs": [],
   "source": [
    "eval_env = gym.make('CarRacing-v0')\n",
    "eval_env = gym.wrappers.resize_observation.ResizeObservation(eval_env, 64)\n",
    "eval_env = gym.wrappers.gray_scale_observation.GrayScaleObservation(eval_env, keep_dim = True)\n",
    "\n",
    "# Using MLP policy change: best_model_save_path='./best_model_MLP_Single/'\n",
    "eval_callback = sb3.common.callbacks.EvalCallback(eval_env, \n",
    "                                                  best_model_save_path='./best_model_CNN_Single/',\n",
    "                                                  log_path=tb_log, \n",
    "                                                  eval_freq=5000,\n",
    "                                                  render=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vMj3hpimQfo0"
   },
   "source": [
    "Train the model for a large number of timesteps (500,000 timesteps will probably work well)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%tensorboard --logdir log_racing_PPO_single"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cNtGLDLUQfo0",
    "outputId": "1bc23c12-4702-4676-88b7-c5e7b632c887"
   },
   "outputs": [],
   "source": [
    "agent.learn(total_timesteps=500000,callback=eval_callback)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Nir4ZmnqQfo1"
   },
   "source": [
    "Save the trained agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CBGtMv04Qfo1"
   },
   "outputs": [],
   "source": [
    "# Using MLP policy change to: agent.save(\"./final_models/final_model_MLP_single\")\n",
    "agent.save(\"./final_models/final_model_CNN_single\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Rxs3h7Owre5T"
   },
   "source": [
    "For memory management delete old agent and environment (assumes variable names - change if required)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uOGiJDrTrdV3"
   },
   "outputs": [],
   "source": [
    "del agent\n",
    "del env\n",
    "del eval_env\n",
    "del render_env"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RQF_FjMEQfo1"
   },
   "source": [
    "###Â Create Image Stack Agent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the CarRacing-v0 environment using wrappers to resize the images to 64 x 64 and change to greyscale. Also add a wrapper to create a stack of 4 frames. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ijzQwZY0Qfo2"
   },
   "outputs": [],
   "source": [
    "# Create Stacked env\n",
    "env = gym.make('CarRacing-v0')\n",
    "env = gym.wrappers.resize_observation.ResizeObservation(env, 64)\n",
    "env = gym.wrappers.gray_scale_observation.GrayScaleObservation(env, keep_dim = True)\n",
    "env = sb3.common.vec_env.DummyVecEnv([lambda: env]) \n",
    "env = sb3.common.vec_env.VecFrameStack(env, n_stack=4)\n",
    "\n",
    "# Separate evaluation env\n",
    "eval_env = gym.make('CarRacing-v0')\n",
    "eval_env = gym.wrappers.resize_observation.ResizeObservation(eval_env, 64)\n",
    "eval_env = gym.wrappers.gray_scale_observation.GrayScaleObservation(eval_env, keep_dim = True)\n",
    "eval_env = sb3.common.vec_env.DummyVecEnv([lambda: eval_env]) \n",
    "eval_env = sb3.common.vec_env.VecFrameStack(eval_env, n_stack=4)\n",
    "\n",
    "# Separate evaluation render env\n",
    "render_env =  wrappers.Monitor(env, \"./gym-results\", force=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "77o0YzOmQfo2"
   },
   "source": [
    "Create an agent that controls the car using a stack of input image frames as the state input. We recommend a PPO agent with the following hyper-parameters (although you can experiment):\n",
    "- learning_rate = 3e-5\n",
    "- n_steps = 512\n",
    "- ent_coef = 0.001\n",
    "- batch_size = 128\n",
    "- gae_lambda =  0.9\n",
    "- n_epochs = 20\n",
    "- use_sde = True\n",
    "- sde_sample_freq = 4\n",
    "- clip_range = 0.4\n",
    "- policy_kwargs = {'log_std_init': -2, 'ortho_init':False},\n",
    "\n",
    "We also recommend enabling **tensorboard** monitoring of the training process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4aAxmNANQfo2"
   },
   "outputs": [],
   "source": [
    "tb_log = './tb_logs_StackFrame_Training/'\n",
    "\n",
    "#policy = 'MlpPolicy'\n",
    "policy = 'CnnPolicy'\n",
    "\n",
    "agent = sb3.PPO(policy, env,\n",
    "                    learning_rate = 3e-5,\n",
    "                    n_steps = 512,\n",
    "                    ent_coef = 0.001,\n",
    "                    batch_size = 128,\n",
    "                    gae_lambda =  0.9,\n",
    "                    n_epochs = 20,\n",
    "                    use_sde = True,\n",
    "                    sde_sample_freq = 4,\n",
    "                    clip_range = 0.4,\n",
    "                    policy_kwargs = {'log_std_init': -2, 'ortho_init':False},\n",
    "                    tensorboard_log=tb_log)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IkOXkC-HQfo2"
   },
   "source": [
    "Examine the actor and critic network architectures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fS-fpmWwQfo3"
   },
   "outputs": [],
   "source": [
    "print(agent.policy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "<span style=\"color:blue\">\n",
    "    \n",
    "## On the actor and critic network\n",
    "    \n",
    "- Only difference in dimension\n",
    "\n",
    "</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p1jObjvdQfo3"
   },
   "source": [
    "Create an evaluation callback that is called every at regular intervals and renders the episode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aL2c4LiOQfo4"
   },
   "outputs": [],
   "source": [
    "eval_callback = sb3.common.callbacks.EvalCallback(eval_env, \n",
    "                                                  best_model_save_path='./best_model_CNN_4Stack/',\n",
    "                                                  log_path=tb_log, \n",
    "                                                  eval_freq=5000,\n",
    "                                                  render=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "s33dR-mgQfo4"
   },
   "source": [
    "Train the model for a large number of timesteps (500,000 timesteps will probably work well)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%tensorboard --logdir ./tb_logs_StackFrame_Training/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lRj1j3xjQfo4"
   },
   "outputs": [],
   "source": [
    "agent.learn(total_timesteps=500000, callback=eval_callback)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "G8j9GrTTQfo5"
   },
   "source": [
    "Save the trained agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Z8ivh_VpQfo5"
   },
   "outputs": [],
   "source": [
    "# Using MLP policy change to: agent.save(\"./final_models/final_model_CNN_4Stack\")\n",
    "agent.save(\"./final_models/final_model_CNN_4Stack\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For memory management delete old agent and environment (assumes variable names - change if required)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del agent\n",
    "del env\n",
    "del eval_env\n",
    "del render_env"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bWZrzrZ3Qfo5"
   },
   "source": [
    "###Â Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DRzc_-yOQfo5"
   },
   "source": [
    "Load the single image saved agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XyfQmsF6Qfo6"
   },
   "outputs": [],
   "source": [
    "# Add code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setup the single image environment for evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-8X8r5S5Qfo6"
   },
   "source": [
    "Evaluate the agent in the environment for 30 episodes, rendering the process. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hrMN5vOeQfo6"
   },
   "outputs": [],
   "source": [
    "# Add code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "y6K6TXdCQfo6"
   },
   "source": [
    "For memory management delete the single image agent (assumes variable names - change if required)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yAeeHAKVQfo6"
   },
   "outputs": [],
   "source": [
    "del agent\n",
    "del eval_env"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "X1y0UngLQfo6"
   },
   "source": [
    "Load the image stack agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lCxi6SvMQfo7"
   },
   "outputs": [],
   "source": [
    "# Add code here \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set up the image stack environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Tr8lGKjpQfo7"
   },
   "source": [
    "Evaluate the agent in the environment for 30 episodes, rendering the process. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LgbbH_UkQfo7"
   },
   "outputs": [],
   "source": [
    "# Add code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reflection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WRCxNiPqQfo7"
   },
   "source": [
    "Reflect on which  agent performs better at the task, and the training process involved (max 200 words)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "machine_shape": "hm",
   "name": "WS3 Going The Distance.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "164.571px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
